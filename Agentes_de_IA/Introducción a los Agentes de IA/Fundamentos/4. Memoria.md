
---

# Memoria en Agentes de IA y Modelos de Lenguaje (LLM)

La memoria es un componente esencial para que un agente de IA pueda comportarse de forma coherente, útil y contextualizada. Un modelo de lenguaje por sí solo no “recuerda” ninguna conversación pasada: solo procesa el texto que recibe en ese momento.  
Por eso, alrededor del modelo se añaden **sistemas de memoria** que permiten extender sus capacidades.

En este documento explicamos los tres grandes tipos de memoria utilizados en agentes basados en LLM:

1. **Memoria de conversación** (corto plazo / trabajo)
    
2. **Memoria de largo plazo** (memoria del usuario, memoria episódica)
    
3. **Memoria de conocimiento o de dominio** (bases de datos, repositorios externos, vectores, wikis, etc.)
    

---

# 1. Memoria de conversación (corto plazo o working memory)

La memoria de conversación es el contexto inmediato que el modelo recibe como entrada.  
Es limitada y volátil: si no se reenvía en el prompt, el modelo la “olvida”.

### Características clave

- Se basa en los **tokens** incluidos en el mensaje actual.
    
- Se pierde cuando no se incluye en la siguiente petición.
    
- Permite mantener un diálogo coherente a lo largo de varias interacciones.
    
- Es la memoria menos estable y la más barata.
    

En un agente, esta memoria suele gestionarse automáticamente concatenando los últimos mensajes o resumiéndolos para mantenerlos dentro del límite de tokens.

### Ejemplo simplificado

```python
historial = []

def enviar_mensaje(mensaje):
    historial.append({"role": "user", "content": mensaje})

    # se envía al LLM solo el resumen o los últimos turnos
    contexto = historial[-4:]  # memoria de trabajo

    respuesta = llm.call(messages=contexto)
    historial.append({"role": "assistant", "content": respuesta})
    return respuesta
```

---

# 2. Memoria de largo plazo

La memoria de largo plazo permite que el agente recuerde información estable en el tiempo.  
Aquí distinguimos **dos subtipos importantes**:

## 2.1. Memoria del usuario

Almacena datos estables sobre la persona:

- preferencias (“prefiero respuestas técnicas”, “respóndeme en español”),
    
- estilo de comunicación,
    
- proyectos recurrentes,
    
- contexto personal relevante para futuras sesiones.
    

No está pensada para conversaciones, sino para **rasgos persistentes**.

### Ejemplo conceptual

```json
{
  "tema": "preferencias_usuario",
  "nombre": "estilo_respuesta",
  "dato": "prefiere explicaciones pedagógicas orientadas a ciberseguridad"
}
```

El agente consulta esta memoria antes de generar la respuesta.

## 2.2. Memoria episódica

Registra eventos importantes que han ocurrido en sesiones anteriores para poder retomarlos después:

- proyectos en marcha,
    
- tareas pendientes,
    
- resúmenes de sesiones completas,
    
- pasos realizados en investigaciones largas.
    

Funciona igual que la memoria humana episódica: recuerda hechos, no reglas permanentes.

Ejemplo:

```json
{
  "fecha": "2025-01-05",
  "evento": "El usuario generó y guardó un script para escanear puertos con Python."
}
```

---

# 3. Memoria de conocimiento o de dominio

Es un tipo de memoria diseñada para almacenar información externa, técnica o especializada:

- repositorios,
    
- bases de datos,
    
- documentación,
    
- wikis internas,
    
- embeddings/vector stores (Pinecone, Chroma, Weaviate),
    
- PDFs indexados,
    
- bases de conocimiento corporativas.
    

El modelo no “recuerda” esta información; la **consulta bajo demanda** mediante retrieval (RAG).

### Funcionamiento

1. El usuario formula una pregunta.
    
2. El módulo de búsqueda recupera documentos relevantes.
    
3. Los documentos se añaden al prompt.
    
4. El LLM genera la respuesta basándose en esos datos.
    

### Ejemplo en código utilizando RAG básico

```python
from sentence_transformers import SentenceTransformer, util
import torch

documentos = [
    "El servidor usa Nginx y corre sobre Ubuntu 22.",
    "La API de pagos requiere un token JWT válido.",
    "El proceso de hardening incluye CIS benchmarks."
]

modelo = SentenceTransformer("all-MiniLM-L6-v2")
embeddings = modelo.encode(documentos, convert_to_tensor=True)

def consultar_memoria(pregunta):
    embedding_pregunta = modelo.encode(pregunta, convert_to_tensor=True)
    similitud = util.cos_sim(embedding_pregunta, embeddings)
    idx = torch.argmax(similitud)
    return documentos[idx]

pregunta = "¿qué servidor se usa?"
print(consultar_memoria(pregunta))
```

Esto devuelve el documento más relevante sin necesidad de que el LLM lo memorice.

---

# 4. Cómo interactúan estos tres tipos de memoria

Los agentes modernos combinan las tres capas para comportarse como un sistema coherente:

|Tipo|Qué aporta|
|---|---|
|**Memoria de conversación**|Coherencia inmediata en un chat|
|**Memoria del usuario / episódica**|Continuidad entre sesiones|
|**Memoria de conocimiento**|Datos externos especializados|

El LLM recibe un prompt compuesto por:

1. información del usuario,
    
2. contexto relevante de memoria episódica,
    
3. datos recuperados de memoria de dominio,
    
4. y los últimos mensajes de conversación.
    

Así el modelo razona con toda la información unificada.

---

# 5. ¿Por qué es importante en ciberseguridad?

Un agente con un buen sistema de memoria puede:

- recordar el entorno del laboratorio,
    
- almacenar resultados históricos de escaneos,
    
- mantener un conjunto de pasos en una investigación forense,
    
- acceder a documentación de exploits o CVEs de forma contextual,
    
- cargar repositorios completos (PoCs, scripts, pruebas),
    
- reducir errores de alucinación consultando fuentes reales.
    

En ciberseguridad la memoria no es un extra: es una necesidad para mantener precisión, coherencia y trazabilidad.

Ejemplo práctico:

```python
memoria_episodica["analisis_443"] = {
    "fecha": "2025-12-10",
    "servicio": "HTTPS",
    "version_detectada": "Apache 2.4.58",
    "posible_vulnerabilidad": "CVE-2024-38475"
}
```

---

# 6. Resumen

Los agentes avanzados no se basan solo en un LLM.  
Necesitan un **sistema de memoria híbrido** formado por:

- **Memoria de conversación** → contexto inmediato.
    
- **Memoria de largo plazo** → información persistente del usuario + eventos importantes.
    
- **Memoria de conocimiento** → bases de información externas y técnicas (RAG).
    

Cuando se combinan correctamente, el agente puede trabajar de manera estable, recordar proyectos, acceder a conocimiento especializado y comportarse de forma mucho más cercana a un asistente experto real.

---
