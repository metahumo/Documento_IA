
---
# Mini proyecto de Machine Learning — Regresión lineal sobre `housing.csv`

En este documento vamos a construir paso a paso un pequeño proyecto de **regresión lineal** usando el dataset `housing.csv`. Explicaremos cada bloque del notebook y al final incluimos el **código completo en un solo archivo `.py`** listo para ejecutar.

[Ver estructura del proyecto](./Mini%20proyectos%20de%20ML/regresion_lineal_housing)

---

## Requisitos y preparación del entorno

### 1.1. Archivo CSV

Debemos tener en el directorio donde se ejecute el script un archivo `housing.csv`. Asegurémonos de que ese archivo esté colocado junto al script `.py` antes de ejecutar.

[Recurso web para obtener archivo csv](https://www.kaggle.com/datasets/camnugent/california-housing-prices?resource=download)

### 1.2. Crear un entorno virtual (recomendado)

Para Windows (desde la terminal PowerShell):

```powershell
python -m venv env  
.\env\Scripts\Activate.ps1
```

En macOS / Linux:

```bash
python3 -m venv env
source env/bin/activate
```

### 1.3. Instalar dependencias

```bash
pip install pandas scikit-learn matplotlib seaborn
```

---

## 2. Flujo general del proyecto (visión global)

1. **Carga y exploración de datos** — inspeccionar `head()`, `info()`, `describe()`, distribuciones y mapas.
    
2. **Limpieza** — eliminar o imputar valores ausentes (en el notebook se hace `dropna()`).
    
3. **Codificación de categóricas** — `get_dummies()` para `ocean_proximity`.
    
4. **Ingeniería de características** — crear ratios útiles (ej.: `bedroom_ratio = total_bedrooms / total_rooms`).
    
5. **Separación en features (X) y etiqueta (y)** — `median_house_value` es la etiqueta.
    
6. **Train/Test split** — con `train_test_split`.
    
7. **Entrenamiento** — `LinearRegression`.
    
8. **Predicción y evaluación** — `R^2`, MSE, RMSE y comparación de predicciones vs valores reales.
    
9. **Escalado** — `StandardScaler` aplicado correctamente (fit en train, transform en test).
    
10. **Visualización** — histogramas, scattermaps, heatmap de correlación y gráficos de predicción.
    

---

## 3. Explicación paso a paso (traduciendo y ampliando el `.ipynb`)

### 3.1. Carga y examen inicial

- `pd.read_csv("housing.csv")` carga el dataset.
    
- `head()`, `info()` y `describe()` nos permiten entender tamaño, tipos y valores faltantes.
    
- `hist()` con `bins=30` nos da idea de distribuciones; un `scatterplot` geográfico (latitude/longitude) coloreado por `median_house_value` ayuda a ver concentración espacial de valores.
    

### 3.2. Tratamiento de valores NA

En el notebook original se hace `datos_na = datos.dropna()`. Eso elimina filas con NA — es una estrategia simple y válida si no perdemos demasiados datos. Alternativa: imputación (`SimpleImputer`) si preferimos conservar las filas.

### 3.3. Codificación de `ocean_proximity`

Usamos `pd.get_dummies(datos_na["ocean_proximity"], dtype=int)` y la concatenamos con el dataframe. Después eliminamos la columna original `ocean_proximity`. Esto produce columnas binarias para cada categoría (one-hot encoding).

### 3.4. Correlaciones y nueva característica

- Calculamos `datos_na.corr()` y representamos la matriz con `seaborn.heatmap` para ver qué variables correlacionan con `median_house_value`.
    
- Creamos `bedroom_ratio = total_bedrooms / total_rooms` como característica nueva; volvemos a observar la matriz de correlación para ver su impacto.
    

### 3.5. Separación X / y y división en conjuntos

- `X = datos_na.drop(["median_house_value"], axis=1)`
    
- `y = datos_na["median_house_value"]`
    
- `train_test_split(X, y, test_size=0.2, random_state=42)` para reproducibilidad.
    

### 3.6. Entrenamiento con `LinearRegression`

- `LinearRegression().fit(X_train, y_train)`
    
- Predicción: `y_pred = modelo.predict(X_test)`
    

### 3.7. Evaluación

- `modelo.score(X_train, y_train)` y `modelo.score(X_test, y_test)` — R² en train y test para detectar sobreajuste.
    
- `mean_squared_error(y_test, y_pred)` y `rmse = sqrt(mse)`.
    

### 3.8. Escalado (corrección respecto al notebook)

En el notebook original se hace `X_pru_esc = scaler.fit_transform(X_pru)` — **esto es incorrecto** porque ajusta el `scaler` con datos de test. Corregimos:

- `scaler.fit(X_train)` y luego `X_train_scaled = scaler.transform(X_train)` y `X_test_scaled = scaler.transform(X_test)`.
    

---

## 4. Visualización de gráficas en Python

En este proyecto crearemos las siguientes gráficas:

- **Histogramas** de todas las variables numéricas (`datos.hist()`).
    
- **Mapa de dispersión geográfico**: `latitude` vs `longitude` coloreado por `median_house_value` y con tamaño según `population`.
    
- **Heatmap** de la matriz de correlaciones para identificar relaciones fuertes.
    
- **Scatter** `median_income` vs `median_house_value` para ver dependencia.
    
- **Scatter de predicción vs valor real**: gráfico `y_test` vs `y_pred` con línea `y=x` para evaluar cómo se alinean las predicciones.
    

Usaremos `matplotlib` y `seaborn` para todas ellas.

---

## 5. Código completo en Python (`regresion_lineal_housing.py`)

```python
# regresion_lineal_housing.py
"""
Script: regresion_lineal_housing.py
Descripción: flujo completo de ML (exploración, limpieza, codificación, entrenamiento y evaluación)
Requisitos: housing.csv en el mismo directorio
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sb
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler

def main():
    # 1) Carga de datos
    datos = pd.read_csv("housing.csv")
    print("Primeros registros:")
    print(datos.head())
    print("\nInfo general:")
    print(datos.info())
    print("\nDescripción numérica:")
    print(datos.describe())

    # 2) Visualizaciones iniciales
    plt.figure(figsize=(12,6))
    datos.hist(figsize=(15,8), bins=30, edgecolor="black")
    plt.suptitle("Histogramas de variables numéricas")
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.show()

    # Mapa de dispersión geográfico
    plt.figure(figsize=(10,8))
    sb.scatterplot(x="latitude", y="longitude", data=datos,
                   hue="median_house_value", palette="coolwarm",
                   size="population", sizes=(10, 300), legend="brief")
    plt.title("Mapa geográfico: latitude vs longitude (color: median_house_value)")
    plt.show()

    # 3) Manejo de valores NA (dropna en este ejercicio)
    datos_na = datos.dropna()
    print("\nTras dropna():")
    print(datos_na.info())

    # 4) One-hot encoding de 'ocean_proximity'
    dummies = pd.get_dummies(datos_na["ocean_proximity"], dtype=int)
    datos_na = datos_na.join(dummies)
    datos_na = datos_na.drop(["ocean_proximity"], axis=1)

    # 5) Ingeniería de características
    datos_na["bedroom_ratio"] = datos_na["total_bedrooms"] / datos_na["total_rooms"]

    # 6) Matriz de correlación
    plt.figure(figsize=(15,8))
    corr = datos_na.corr()
    sb.heatmap(corr, annot=True, cmap="YlGnBu")
    plt.title("Matriz de correlación")
    plt.show()

    print("\nCorrelaciones con 'median_house_value':")
    print(corr["median_house_value"].sort_values(ascending=False))

    # 7) Separar X e y
    X = datos_na.drop(["median_house_value"], axis=1)
    y = datos_na["median_house_value"]

    # 8) Train/Test split con semilla para reproducibilidad
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)

    # 9) Escalado (fit en train, transform en train y test)
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # (Opcional) convertir de vuelta a DataFrame para inspección
    # X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)

    # 10) Entrenamiento del modelo
    modelo = LinearRegression()
    modelo.fit(X_train_scaled, y_train)

    # 11) Predicción
    y_pred = modelo.predict(X_test_scaled)

    # 12) Evaluación
    r2_train = modelo.score(X_train_scaled, y_train)
    r2_test = modelo.score(X_test_scaled, y_test)
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)

    print(f"\nR^2 (train): {r2_train:.4f}")
    print(f"R^2 (test):  {r2_test:.4f}")
    print(f"MSE (test):  {mse:.4f}")
    print(f"RMSE (test): {rmse:.4f}")

    # Comparativa Predicción vs Real (primeras filas)
    comparativa = pd.DataFrame({"Prediccion": y_pred, "Valor Real": y_test.values})
    print("\nComparativa (primeras filas):")
    print(comparativa.head(10))

    # 13) Gráfica Predicción vs Real
    plt.figure(figsize=(8,6))
    plt.scatter(y_test, y_pred, alpha=0.6)
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)
    plt.xlabel("Valor Real")
    plt.ylabel("Predicción")
    plt.title("Predicción vs Valor Real")
    plt.grid(True)
    plt.show()

if __name__ == "__main__":
    main()
```

> Observaciones sobre el script:
> 
> - Hemos añadido `random_state=42` para reproducibilidad.
>     
> - Usamos `StandardScaler` correctamente: `fit` solo sobre los datos de entrenamiento y `transform` sobre ambos.
>     
> - Mantenemos las visualizaciones para inspección del flujo.
>     

---

## 6. Salida esperada (ejemplo) y comentarios

La salida real variará en función del contenido exacto de `housing.csv` y el muestreo aleatorio del `train_test_split` si no fijamos `random_state`. Con el `random_state=42` los valores serán reproducibles. A continuación mostramos una salida **de ejemplo** (fragmentada) que ilustra lo que veríamos al ejecutar el script:

```
Primeros registros:
   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  population  households  median_income  median_house_value ocean_proximity
0 -122.23    37.88        41.0            880.0        129.0           322.0       126.0         8.3252         452600.0          NEAR BAY
1 -122.22    37.86        21.0            7099.0       1106.0          2401.0      1138.0         8.3014         358500.0          NEAR BAY
...
Info general:
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 20640 entries, 0 to 20639
...
Descripción numérica:
         longitude       latitude  ...  median_income  median_house_value
count  20640.000000  20640.000000  ...  20640.000000        20640.000000
mean   -119.569704     35.631861  ...      3.870671       206855.816909
std       2.003532      2.135952  ...      1.899822       115395.615874
min    -124.350000     32.540000  ...      0.499900        14999.000000
max    -114.310000     41.950000  ...     15.000100       500001.000000

Tras dropna():
<class 'pandas.core.frame.DataFrame'>
RangeIndex: N entries, 0 to N-1
Columns: ... 
# (información tras eliminar filas NA)

Correlaciones con 'median_house_value':
median_income         0.688...
median_income_squared (si existiera) ...
...

R^2 (train): 0.65xx
R^2 (test):  0.62xx
MSE (test):  <valor numérico>
RMSE (test): <valor numérico>

Comparativa (primeras filas):
       Prediccion    Valor Real
123   187000.43     185000.00
456   123456.78     120000.00
...
```

**Comentarios sobre expectativas**:

- No necesariamente esperamos R² = 1: con datos reales el ajuste será imperfecto.
    
- Si el dataset tiene valores extremos o variables no lineales fuertes, la regresión lineal simple puede no ser suficiente y convendrá usar modelos más flexibles (Ridge/Lasso, RandomForest, XGBoost, etc.).
    
- El RMSE nos aporta la magnitud típica del error en las mismas unidades que `median_house_value` (euros/dólares según dataset).
    

---

## 7. Notas pedagógicas y mejoras posteriores

- **Imputación vs eliminación**: en lugar de `dropna()`, podemos usar `SimpleImputer(strategy="median")` para no perder datos.
    
- **Regularización**: si observamos sobreajuste, usar `Ridge` o `Lasso`.
    
- **Feature engineering**: además de `bedroom_ratio`, podemos crear `rooms_per_household`, `population_per_household`, `income_categories` (discretizar `median_income`).
    
- **Validación cruzada**: usar `cross_val_score` para estimar mejor rendimiento.
    
- **Pipeline**: usar `ColumnTransformer` y `Pipeline` para ordenar transformaciones y evitar fugas de datos.
    

---
